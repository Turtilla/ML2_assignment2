{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Convolutions with MIDI\n",
    "\n",
    "In this assignment, you're going to play around with the MIDI notebook we've been building in class.\n",
    "\n",
    "The code should run on mltgpu at the time of submission, but you do not need to use the GPU for this assignment.  (You can if you want to.)\n",
    "\n",
    "When testing on your own machine, in addition to the full PyTorch stack, you'll need the mido module.  Installing the scamp module is necessary if you want to listen to anything. If using Linux, you will have to install fluidsynth.\n",
    "\n",
    "You will use the [lakh](https://colinraffel.com/projects/lmd/) MIDI corpus.  A copy will be placed in the scratch directory of mltgpu; information will be provided via Canvas announcement.\n",
    "\n",
    "This assignment is due on November 1, 2022, at 23:59.  There are **25 points** and **29 bonus points** (!!!) available on this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mido import MidiFile\n",
    "import os\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 -- improve data handling and representation (4 points)\n",
    "\n",
    "Here you will take the `MessageSequence` we created in class and make the following improvements:\n",
    "\n",
    "1. Change the representation so that it can accommodate start and end symbols, as appropriate for your modeling in part 2.\n",
    "\n",
    "2. Allow for the loading of multiple channels (2 or more, possibly randomly selected), with a reasonable cutoff.  To make things simple, you can make the very wrong assumption that every note is of the same duration and therefore aligned one-by-one, and you can thus ignore duration and offset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "class MIDITrackError(Exception):\n",
    "    pass\n",
    "\n",
    "class MessageSequence:\n",
    "    def __init__(self, mid):\n",
    "        self.messages = []\n",
    "        try: \n",
    "            for i in mid.tracks[1]:\n",
    "                #print(i.type)\n",
    "                if i.type in ['note_on', 'note_off']:\n",
    "                    self.messages.append(i)\n",
    "        except IndexError:\n",
    "            raise MIDITrackError\n",
    "        \n",
    "        #calculate note durations\n",
    "        timecounter = 0\n",
    "        notedict = {}\n",
    "        real_sequence = []\n",
    "        for message in self.messages:\n",
    "            timecounter += message.time\n",
    "            if message.type == \"note_on\":\n",
    "                notedict[message.note] = timecounter\n",
    "                \n",
    "            if message.type == \"note_off\":\n",
    "                duration = timecounter - notedict[message.note]\n",
    "                real_sequence.append((message.note, notedict[message.note], message.time, duration))\n",
    "                \n",
    "        self.sequence = real_sequence\n",
    "        \n",
    "    def midi_reencode(self):\n",
    "        reencoded = []\n",
    "        active_notes = {}\n",
    "        timecounter = 0\n",
    "        for (note, timestamp, offset, duration) in self.sequence:\n",
    "            note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0]) #timestamp is tuple item 0\n",
    "            for active_note in note_order:\n",
    "                if active_notes[active_note][0] < timestamp:\n",
    "                    reencoded.append(mido.Message(\"note_off\", \n",
    "                                                  channel=1, \n",
    "                                                  note=active_note, \n",
    "                                                  velocity=95, \n",
    "                                                  time=active_notes[active_note][1]))\n",
    "                    timecounter += active_notes[active_note][1]\n",
    "                    del active_notes[active_note]\n",
    "            reencoded.append(mido.Message(\"note_on\", \n",
    "                                          channel=1, \n",
    "                                          note=note,\n",
    "                                          velocity=95, \n",
    "                                          time=timestamp-timecounter))\n",
    "            active_notes[note] = (timestamp+duration, offset, duration)\n",
    "            timecounter = timestamp\n",
    "\n",
    "            \n",
    "        note_order = sorted(active_notes.keys(), key=lambda x: active_notes[x][0]) #timestamp is tuple item 0\n",
    "        for active_note in note_order:\n",
    "            reencoded.append(mido.Message(\"note_off\", \n",
    "                                          channel=1, \n",
    "                                          note=active_note, \n",
    "                                          velocity=95, \n",
    "                                          time=active_notes[active_note][1]))\n",
    "            del active_notes[active_note]\n",
    "            \n",
    "        return reencoded\n",
    "    \n",
    "    def vector_encode(self):\n",
    "        note_db = functional.one_hot(torch.arange(0, 128))\n",
    "        encoded = []\n",
    "        for (note, _, offset, duration) in self.sequence:\n",
    "            note_vec = note_db[note].clone().detach()\n",
    "            if offset > 100:\n",
    "                offset = 100\n",
    "            if duration > 4000:\n",
    "                duration = 4000\n",
    "            \n",
    "            offset = offset/100\n",
    "            duration = duration/4000\n",
    "            \n",
    "            encoded.append(torch.cat((note_vec, torch.Tensor([offset]), torch.Tensor([duration]))))\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your changes and any special motivations for them here (in notebook Markdown):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Convolutional Model (8 points)\n",
    "\n",
    "Replace the model below with a model with the following characteristics:\n",
    "\n",
    "1. It should include an ensemble of parallel 1D-convolutional layers (2 or more)\n",
    "2. The layers should combine into a single output representation.\n",
    "3. The layers should be different (have different kernels, windows, or strides).\n",
    "4. The layers should be able to handle multiple channels. \n",
    "5. The input will be the song representation up to time step n, and the output will be a representation of notes for a single time step across the channels at n+1. (This means that an instance will be prediction of the next note, and a song will have to be run n times to predict n characters.)\n",
    "\n",
    "Training the model will take longer than the n-gram model, especially if you're not using the GPU.\n",
    "\n",
    "You have a free hand in all other aspects of the model, as long as you explain any significant design decisions (i.e., not every minor choice, but ones with real design impact).\n",
    "\n",
    "(A bit of advice: the biggest problem here will be keeping the matrix/tensor dimensions straight...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIModel(nn.Module):\n",
    "    def __init__(self, hidden_layer_size=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(260, hidden_layer_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(hidden_layer_size, 130)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=0)\n",
    "        self.offsig = nn.Sigmoid()\n",
    "        self.dursig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        o = self.linear1(data)\n",
    "        o = self.sigmoid(o)\n",
    "        o = self.linear2(o)\n",
    "        note_output = o[:,0:128]\n",
    "        offset_output = o[:,128]\n",
    "        duration_output = o[:,129]\n",
    "        note_output = self.logsoftmax(note_output)\n",
    "        offset_output = self.offsig(offset_output)\n",
    "        duration_output = self.dursig(duration_output)\n",
    "        return (note_output, offset_output, duration_output)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain your design choices below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Dataset sampling (4 points)\n",
    "\n",
    "Consider how the model is designed above and design a dataset generator capable of producing sample prefixes and next-characters for each time step for each song.  You can replace all the code from the original MIDI notebook with whatever you want.  Consider that there are more and less efficient ways of doing this, and that it may also be worth seeing if it's easier to do this in iterator mode where you can select random prefixes from random songs at each iteration.  You can even choose not to use the torch Dataset class at all, though it means you will have to rewrite the training loop not to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_per_song(song):\n",
    "    vectors = song.vector_encode()\n",
    "    samples = []\n",
    "    for i in range(2, len(vectors)):\n",
    "        samples.append((torch.cat((vectors[i-2], vectors[i-1])), (torch.LongTensor([song.sequence[i][0]]), vectors[i][-2], vectors[i][-1])))\n",
    "        \n",
    "    return samples\n",
    "\n",
    "def generate_samples(songlist):\n",
    "    samples = []\n",
    "    for song in songlist:\n",
    "        samples += generate_samples_per_song(song)\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDINotesDataset(Dataset):\n",
    "    def __init__(self, mididir, maximum=500):\n",
    "        items = os.walk(mididir)\n",
    "        \n",
    "        self.filenames = []\n",
    "        for (directory, _, files) in items:\n",
    "            self.filenames += [os.path.join(directory, x) for x in files]\n",
    "        \n",
    "        mss = []\n",
    "        count = 0\n",
    "        for x in self.filenames:\n",
    "            try:\n",
    "                midifile = MidiFile(x)\n",
    "                ms = MessageSequence(midifile)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            mss.append(ms)\n",
    "            \n",
    "            count += 1\n",
    "            if count == maximum:\n",
    "                break\n",
    "            \n",
    "        self.notes = generate_samples(mss)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.notes[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDISongsDataset(Dataset):\n",
    "    def __init__(self, mididir, maximum=500):\n",
    "        items = os.walk(mididir)\n",
    "        \n",
    "        self.filenames = []\n",
    "        for (directory, _, files) in items:\n",
    "            self.filenames += [os.path.join(directory, x) for x in files]\n",
    "        \n",
    "        mss = []\n",
    "        count = 0\n",
    "        for x in self.filenames:\n",
    "            try:\n",
    "                midifile = MidiFile(x)\n",
    "                ms = MessageSequence(midifile)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            mss.append(ms)\n",
    "            \n",
    "            count += 1\n",
    "            if count == maximum:\n",
    "                break\n",
    "                \n",
    "        self.songs = [[x[0] for x in y.sequence] for y in mss]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.songs[i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe any significant choices you made in designing the mode of access to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Training loop (2 points)\n",
    "\n",
    "Adapt the training loop to the way you organized access to the dataset and to the model you wrote.  Make any other improvements, such as trying out a different optimizer.  Make sure it is possible to vary the batch size as well as the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs=10):\n",
    "    mm = MIDIModel()\n",
    "    optimizer = optim.SGD(mm.parameters(), lr=0.001, momentum=0.9)\n",
    "    note_criterion = nn.NLLLoss()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        loader = DataLoader(data, batch_size=25, shuffle=True, drop_last=True)\n",
    "        for i, o in loader:\n",
    "            #print(i)\n",
    "            #print(i.shape)\n",
    "            #print(o)\n",
    "            optimizer.zero_grad()\n",
    "            (note_output, offset_output, duration_output) = mm(i)\n",
    "            #print(\"no: {}, oo: {}, do: {}\".format(note_output, offset_output, duration_output))\n",
    "            #print(note_output, o[0].reshape((i.shape[0])))\n",
    "            note_loss = torch.exp(-note_criterion(note_output, o[0].reshape((i.shape[0]))))\n",
    "            offset_loss = torch.abs(o[1] - offset_output)\n",
    "            duration_loss = torch.abs(o[2] - duration_output)\n",
    "            #print(\"nl: {}, ol: {}, dl: {}\".format(note_loss, offset_loss, duration_loss))\n",
    "            loss = note_loss + offset_loss + duration_loss\n",
    "            losses.append(sum(loss))\n",
    "            sum(loss).backward()\n",
    "            optimizer.step()\n",
    "        print(\"mean loss in epoch {} is {}\".format(epoch, float(torch.mean(torch.stack(losses)))))\n",
    "    return mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there are any remarks you have on the training loop, put them here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Evaluation (7 points)\n",
    "\n",
    "Actually predicting accuracy of note prediction in a set of songs is probably unlikely to work.  So instead we will calculate the perplexity of your model under different training assumptions (for example, epochs, dropout probability -- if you used dropout -- and/or hidden layer size).  Divide your dataset into training and validation sets and use the validation for the perplexity calculation.  (Note that you are predicting notes across multiple channels, so will have to combine perplexities across the channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here, you can add more notebook cells of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks on your evaluation here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 1 -- \"Music\" (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to properly install [scamp](http://scamp.marcevanstein.com/) to do this bonus. You can rewrite the mode of song generation here to take into account your convolutional process.  Then use scamp to play the (multi-channel/simultaneous note music back).  Try to see if you get any quality improvement at all by using better parameters. (It will probably sound awful no matter what.)  If you want to train on mltgpu and play music on your own computer, you'll have to also write a way to save and load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "# This is just to get the first two notes out of the development song.\n",
    "vecs = x.vector_encode()\n",
    "\n",
    "def generate_music(model, note1, note2, length=30, diversity=5):\n",
    "    note_db = functional.one_hot(torch.arange(0, 128))\n",
    "    newsong = [note1, note2]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            notepair = torch.cat((note1, note2))\n",
    "            fake_batch = torch.stack([notepair] + [torch.randn(260) for _ in range(24)])\n",
    "            (note_output, offset_output, duration_output) = model(fake_batch)\n",
    "            note_output = note_output[0]\n",
    "            offset_output = offset_output[0]\n",
    "            duration_output = duration_output[0]\n",
    "            print(\"note_output: {}\".format(note_output))\n",
    "            notesort = torch.argsort(note_output, descending=True)\n",
    "            print(\"notesort: {}\".format(notesort))\n",
    "            noteset = notesort[:diversity]\n",
    "            print(\"noteset: {}\".format(noteset))\n",
    "            notenum = int(choice(noteset.numpy()))\n",
    "            print(\"notenum: {}\".format(notenum))\n",
    "            note1 = note2\n",
    "            print(\"testgen {} {} {}\".format(note_db[notenum].clone().detach(), offset_output, duration_output))\n",
    "            note2 = torch.cat((note_db[notenum].clone().detach(), torch.Tensor([offset_output]), \n",
    "                                                                               torch.Tensor([duration_output])))\n",
    "            newsong.append(note2)\n",
    "    return newsong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconvert_song(notetensors):\n",
    "    return [(int(torch.argmax(x[0:128])), int(torch.floor(x[128] * 100)), int(torch.floor(x[129] * 4000))) for x in notetensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_back(model_ouptut, starting_time):\n",
    "    sequence = []\n",
    "    for (note, offset, duration) in model_ouptut:\n",
    "        sequence.append((note, starting_time, offset, duration))\n",
    "        starting_time += duration - offset\n",
    "        \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scamp import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = Session().run_as_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using preset Clarinet for clarinet\n"
     ]
    }
   ],
   "source": [
    "clarinet = sess.new_part(\"clarinet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in converted_song:\n",
    "    clarinet.play_note(n[0], 0.8, n[2]/1000)\n",
    "    time.sleep(n[2]/1000 + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks on the quality of the music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 2 - 2D-convolutions (6 points)\n",
    "\n",
    "Define a model as in part 2 that restructures your representation as an ensemble of 2D convolutional models (using the additional dimension to handle multiple MIDI channels).  This will probably require that you rebuild other parts of the pipeline to accommodate it.\n",
    "\n",
    "Do an evaluation of the output in terms of perplexity (and, optionally, musical quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code here (in as many cells as you need):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Part 3 - Durations (20 points)\n",
    "\n",
    "Starting from the song representation, find a way to properly handle durations across multiple channels so that your code is not reliant on an incorrect alignment of the sequence of notes.  Evaluate as in Bonus Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your remarks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a filled-out version of this notebook via Canvas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
